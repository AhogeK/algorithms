<!-- TOC -->
* [动态规划（上）](#动态规划上)
  * [动态规划概述](#动态规划概述)
    * [例题讲解：LCR 125.斐波那契数](#例题讲解lcr-125斐波那契数)
      * [算法思路](#算法思路)
      * [代码实现](#代码实现)
      * [复杂度分析](#复杂度分析)
    * [「记忆化递归」与「动态规划」](#记忆化递归与动态规划)
      * [它们到底是什么关系](#它们到底是什么关系)
      * [什么时候更适合用记忆化递归？](#什么时候更适合用记忆化递归)
      * [什么时候更适合用循环 DP？](#什么时候更适合用循环-dp)
<!-- TOC -->

# 动态规划（上）

> 动态规划（Dynamic Programming，简称 DP），听起来像是一种高深的“编程方式”，但它实际上是一种算法设计思想。

用一句话概括就是：**大事化小，小事化了，且绝不重复做同样的事。**

它核心的哲学是：**通过把原问题分解为相对简单的子问题的方式求解复杂问题**。但与“分治法”不同的是，这些子问题往往是**重叠**的，
动态规划会将每个子问题的解**存起来**（通常存在一个数组或哈希表中），当下次需要这个子问题的结果时，直接查表，而不是重新计算。

这是一种典型的**空间换时间**（Space for Time）的策略。

1. 通俗理解：1+1+1+1 的故事

    Quora 上有一个非常经典的解释：

    > **问：** 纸上写着 “ $1+1+1+1+1+1+1+1$”，问你结果是多少？\
    > **答：** 你数了一下，说是 $8$。
    >
    > **问：** 我在后面又加了一个 “ $+1$”，现在是多少？\
    > **答：** 你会立刻说是 $9$。
    >
    > **问：** 为什么你这次算出 $9$ 这么快？不需要重新数一遍前面的 $8$ 个 $1$ 吗？\
    > **答：** 因为我**记住了**前面的结果是 $8$，只需要 $8+1$ 就行了。

    这就是动态规划的核心：**记住所求过的解（Memoization/Tabulation），避免重复计算**。

2. 动态规划的两大核心性质

    如果一个问题能用动态规划解决，它通常具备以下两个特征：

    ① 重叠子问题 (Overlapping Subproblems)

    在求解过程中，同一个子问题会被多次调用。

    * **反例**：归并排序。虽然它分成了子数组，但左边的数组和右边的数组完全没关系，不重叠。这是“分治”。
    * **正例**：斐波那契数列。计算 $f(5)$ 需要 $f(4)$ 和 $f(3)$；计算 $f(6)$ 需要 $f(5)$ 和 $f(4)$。这里 $f(4)$ 就被重复利用了。

    ② 最优子结构 (Optimal Substructure)

    原问题的“最优解”包含其子问题的“最优解”。

    * **例子**：在这个图中求 $A \to C$ 的最短路径。如果最短路径经过 $B$，那么 $A \to B$ 的路径也必须是 $A$ 到 $B$ 之间的最短路径。
    * **意义**：我们可以通过组合子问题的最优解来构建原问题的最优解。

3. 解题四部曲

    做 DP 题时，通常按照这四个步骤思考：

    1. **定义状态（State Definition）：**\
       最重要的一步。你需要定义数组 `dp[i]` 或者 `dp[i][j]` 到底代表什么含义？

        * *例如：* `dp[i]` 表示爬到第 $i$ 阶楼梯的方法总数。

    2. **推导状态转移方程（State Transition Equation）：**\
       这是 DP 的灵魂。如何从已知的小规模状态推导出当前状态？

        * *例如：* 要到第 $i$ 阶，只能从第 $i-1$ 阶迈一步，或从 $i-2$ 阶迈两步。
        * 方程： $dp[i] = dp[i-1] + dp[i-2]$

    3. **初始化（Initialization/Base Case）：**\
       最基础的状态是多少？防止数组越界，也为递推提供起点。

        * *例如：* $dp = 0, dp = 1, dp = 2$。

    4. **确定遍历顺序（Traversal Order）：**\
       是从前向后算，还是从后向前算？

        * *例如：* 想算 $dp[i]$，必须先知道 $dp[i-1]$，所以要从 $i=3$ 开始从小到大遍历。

4. 举个例子：斐波那契数列

    计算第 $n$ 个斐波那契数。

    ❌ 暴力递归（Naive Recursion）

    ```
    int fib(int n) {
        if (n <= 1) return n;
        return fib(n - 1) + fib(n - 2);
    }
    ```

    * **问题**：计算 `fib(5)` 会计算 `fib(4)` 和 `fib(3)`，计算 `fib(4)` 又要算 `fib(3)`... `fib(3)` 被重复计算了多次。
    * **复杂度**：时间复杂度爆炸，约为 $\mathcal{O}(2^n)$。

    ✅ 动态规划（Dynamic Programming）

    我们开一个数组 `dp` 记录结果。

    ```
    int fib(int n) {
        if (n <= 1) return n;
        int[] dp = new int[n + 1];
        // 初始化
        dp[0] = 0;
        dp[1] = 1;
        // 状态转移
        for (int i = 2; i <= n; i++) {
            dp[i] = dp[i - 1] + dp[i - 2];
        }
        return dp[n];
    }
    ```

    * **优化**：每个状态只算一次。
    * **复杂度**：时间复杂度降为 $\mathcal{O}(n)$，空间复杂度 $\mathcal{O}(n)$（可进一步优化为 $\mathcal{O}(1)$）。

**当你发现一个问题可以分解，且分解后的子问题会重复出现时，请立刻想到：DP 大法好。**

## 动态规划概述

### 例题讲解：LCR 125.[斐波那契数](https://leetcode.cn/problems/fei-bo-na-qi-shu-lie-lcof)

#### 算法思路

1. **朴素递归（不推荐 ❌）**

    如果直接翻译公式写递归，计算 $F(n)$ 需要计算 $F(n-1)$ 和 $F(n-2)$，这会产生大量的重复计算。时间复杂度是指数级的 $\mathcal{O}(2^n)$，对于 $n=100$ 绝对会超时。

2. **记忆化搜索（自顶向下 ✅）**

    使用一个数组记录已经计算过的 $F(n)$，避免重复计算。这可以将复杂度降为 $\mathcal{O}(n)$，但需要额外的栈空间。

3. **迭代动态规划（自底向上 ✅）**

    我们可以从 $F(0)$ 和 $F(1)$ 开始，逐步计算 $F(2), F(3), \dots, F(n)$。\
    状态转移方程为：

    $$dp[i] = (dp[i-1] + dp[i-2]) \pmod{1000000007}$$

    这也是题目要求的核心思路。

4. **滚动数组优化（执行最快 🚀）**

    观察上述转移方程，计算 $dp[i]$ 只需要用到 $dp[i-1]$ 和 $dp[i-2]$ 这两个状态。这意味着我们不需要维护一个长度为 $n+1$ 的数组，只需要 **两个变量** 滚动更新即可。

    * 设 $a$ 代表 $F(i-2)$
    * 设 $b$ 代表 $F(i-1)$
    * 计算 $sum = (a + b) \\% MOD$，这里的 $sum$ 就是 $F(i)$
    * **滚动更新**：下一次循环时，原先的 $F(i-1)$ 变为 $F(i-2)$，原先的 $F(i)$ 变为 $F(i-1)$。即： $a \leftarrow b$， $b \leftarrow sum$。

    这种方法将空间复杂度降低到了极致的 $\mathcal{O}(1)$。

**🗝️ 核心知识点与技巧**

* **动态规划 (DP)**：将大问题分解为重叠子问题，通过状态转移求解。
* **空间优化 (滚动数组)**：当状态转移只依赖于前几个状态时，可以使用有限的变量代替数组，极大地节省内存。
* **取模运算性质**： $(a + b) \pmod m = ((a \pmod m) + (b \pmod m)) \pmod m$。在本题中，由于中间结果不会超过 `Integer.MAX_VALUE`，直接相加后取模即可。

#### 代码实现

```java
public class Fib {
    public int fib(int n) {
        if (n < 2) return n;
        int a = 0;
        int b = 1;
        final int MOD = 1000000007;
        for (int i = 2; i <= n; i++) {
            int sum = (a + b) % MOD;
            a = b;
            b = sum;
        }
        return b;
    }
}
```

#### 复杂度分析

* **时间复杂度**： $\mathcal{O}(n)$
    * 代码包含一个从 $2$ 到 $n$ 的循环，循环内部的操作（加法、取模、赋值）都是常数时间 $\mathcal{O}(1)$。因此总时间与 $n$ 成正比。
* **空间复杂度**： $\mathcal{O}(1)$
    * 我们只使用了 `a`, `b`, `sum`, `i` 等几个有限的变量，没有使用随 $n$ 增长的数组或递归栈空间。

### 「记忆化递归」与「动态规划」

> “记忆化递归”和“动态规划（DP）”本质上在做同一件事：把一个问题拆成重叠子问题，并把子问题答案缓存起来，避免重复计算。
> 它们最大的区别通常不在“思想”，而在“写法”和“计算顺序”。

#### 它们到底是什么关系

记忆化递归可以理解成“自顶向下的 DP”（Top-down DP）：你按人类直觉写递归，把每个状态的答案存进 `memo`，下次再遇到同一状态就直接返回。

动态规划（常指循环写法）更像“自底向上的 DP”（Bottom-up DP）：你先确定状态的计算顺序，用 `dp` 数组从小到大（或按拓扑序）把所有需要的状态填出来，最后取目标状态。

很多教材会说：

* 记忆化递归 = DP 思想 + 递归 + 缓存
* 循环 DP = DP 思想 + 表格填充（tabulation）

**一个最小例子：斐波那契**

递推关系是：

$f(n)=f(n-1)+f(n-2)$

*记忆化递归（Top-down）*

* 你只会计算“真正用到的状态”
* 但有函数调用开销，并且要吃递归栈深度（比如 $n$ 很大时可能爆栈）

```java
Long[] memo = new Long[N];

long f(int n) {
    if (n <= 1) return n;
    if (memo[n] != null) return memo[n];
    
    return memo[n] = f(n - 1) + f(n - 2); // 赋值并返回
}
```

*循环 DP（Bottom-up）*

* 状态计算顺序清晰，通常更稳
* 一般不会爆栈，常数也更小

```java
long f(int n) {
    if (n <= 1) return n;
    long dp0 = 0, dp1 = 1;
    for (int i = 2; i <= n; i++) {
        long sum = dp0 + dp1;
        dp0 = dp1;
        dp1 = sum;
    }
    return dp1;
}
```

两者时间复杂度都能做到 $\mathcal{O}(n)$；区别更多体现在工程表现（栈、常数、可控性）上。

#### 什么时候更适合用记忆化递归？

下面这些场景，记忆化递归往往更顺手，甚至更不容易写错：

* 状态转移天然是“从目标往回问”的，比如区间 DP、树形 DP、博弈 DP（从根/目标状态递归下去很自然）
* 可达状态很稀疏：理论上状态空间很大，但实际只会访问其中一小部分（Top-down 只算访问到的）
* 你一时难以确定一个好用的“填表顺序”（Bottom-up 需要确保依赖已算完）

#### 什么时候更适合用循环 DP？

循环 DP 通常更适合追求稳定和性能的场合：

* 依赖关系很规则（例如线性、网格、背包），填表顺序简单明确
* 数据规模大，递归层数可能很深（避免爆栈）
* 需要进一步优化（滚动数组、位运算优化、严格控制内存布局等）

**一个实用的判断标准**

*如果你已经写出了“状态 + 转移”，但还没想清楚遍历顺序，那就先写记忆化递归把逻辑跑通；等你确认依赖关系和访问范围后，再考虑把它“翻译”成循环 DP 来提速和增强鲁棒性。*

***

[返回](../README.md)